{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe42a701-b88e-481b-8066-906e67cf1467",
   "metadata": {},
   "source": [
    "# Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb5ac0f-7d4a-411c-bfbd-8f3383e71f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer \n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd33a6f-8f53-4f18-9413-3077bf45d25d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Select the model \n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6b383c-1077-4074-a0b1-89f511700381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    'I have been waiting for a Huggingface course my whole life.', \n",
    "    'I hate this so much!',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47853925-f40e-4dc8-a04e-46bb4a98924e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2031,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='pt') \n",
    "print(inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ee8b2b1-1976-448d-89f4-f65e01e487f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55023fe7-674b-4952-80d6-80909432ead9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(checkpoint) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b283e65-3b5e-4ed6-82a1-2c4d68da3704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model(**inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e675d17c-bec6-4790-9287-cc610d9a3653",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1ad2a-0e17-4266-bfc9-033e62c4811b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
