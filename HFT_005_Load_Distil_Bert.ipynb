{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c7ffbdc-bf05-48d0-a323-2c9fcc78f1e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 22:27:08.990069: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9360] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-17 22:27:08.990170: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-17 22:27:08.990215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1537] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93770190-c886-4257-a0cb-bbabd8bde58e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e01d1e10-c547-4be0-a60f-5fbadb1cd8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example     of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script. John Doe cut off Trump.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b7e83b1-ed5d-42d7-911e-937bcaa48645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = question_answerer(question=\"What is a good example of a question answering dataset?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d1e0a8-43cc-4f4d-a364-ecbed088f604",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'SQuAD dataset', score: 0.5274, start: 151, end: 164\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21daa040-5739-4383-88fd-18b8892b7888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_answerer.save_pretrained(\"my_crazy_bert_2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89072f11-9bb2-4aba-a205-d6bb3eec240f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = question_answerer(question=\"Who cut off Trump?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a51fd102-64d4-453c-ae26-9cd02fd1649d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'John Doe', score: 0.9976, start: 338, end: 346\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
